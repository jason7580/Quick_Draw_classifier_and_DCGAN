{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TtWmhV4zJTD",
        "outputId": "2748dad6-d56d-4ad8-bb69-6b26b7b34a1d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3UkON5vzSu3",
        "outputId": "84895ba9-0867-4a22-e5be-08d23c9f62d0"
      },
      "source": [
        "!pip3 install h5py\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.19.4)\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN7CT0mXzTLp",
        "outputId": "b5aa74e3-391f-428a-e0b9-3f392668a22f"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import cv2\n",
        "import ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageDraw\n",
        "!pip install \"dask[bag]\"\n",
        "from dask import bag\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "%matplotlib inline\n",
        "from keras.backend import clear_session\n",
        "from keras.layers import Activation, Reshape\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, BatchNormalization, Dropout, Flatten\n",
        "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D, Reshape, Flatten\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.backend import clear_session\n",
        "from keras.models import load_model\n",
        "\n",
        "np.random.seed(0)\n",
        "class_paths = glob('/content/drive/My Drive/AI/input/train/*.csv')\n",
        "class_paths2 = pd.read_csv('/content/drive/My Drive/AI/input/generator_input.csv')\n",
        "numstonames = {i: v[39:-4].replace(\" \", \"_\") for i, v in enumerate(class_paths)}\n",
        "print(numstonames)\n",
        "namestonums = {v:k for k,v in numstonames.items()}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dask[bag] in /usr/local/lib/python3.6/dist-packages (2.12.0)\n",
            "Requirement already satisfied: toolz>=0.7.3; extra == \"bag\" in /usr/local/lib/python3.6/dist-packages (from dask[bag]) (0.11.1)\n",
            "Collecting partd>=0.3.10; extra == \"bag\"\n",
            "  Downloading https://files.pythonhosted.org/packages/44/e1/68dbe731c9c067655bff1eca5b7d40c20ca4b23fd5ec9f3d17e201a6f36b/partd-1.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: cloudpickle>=0.2.1; extra == \"bag\" in /usr/local/lib/python3.6/dist-packages (from dask[bag]) (1.3.0)\n",
            "Collecting fsspec>=0.6.0; extra == \"bag\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/80/72ac0982cc833945fada4b76c52f0f65435ba4d53bc9317d1c70b5f7e7d5/fsspec-0.8.5-py3-none-any.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 7.4MB/s \n",
            "\u001b[?25hCollecting locket\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/22/3c0f97614e0be8386542facb3a7dcfc2584f7b83608c02333bced641281c/locket-0.2.0.tar.gz\n",
            "Building wheels for collected packages: locket\n",
            "  Building wheel for locket (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for locket: filename=locket-0.2.0-cp36-none-any.whl size=4042 sha256=f67d92bb8643b2daa3de85829f309c9366a703218af15dfd3bc921753bf95336\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/1e/e8/4fa236ec931b1a0cdd61578e20d4934d7bf188858723b84698\n",
            "Successfully built locket\n",
            "Installing collected packages: locket, partd, fsspec\n",
            "Successfully installed fsspec-0.8.5 locket-0.2.0 partd-1.1.0\n",
            "{0: 'ear', 1: 'diamond', 2: 'clock', 3: 'airplane', 4: 'camera', 5: 'door', 6: 'hammer', 7: 'cake', 8: 'bird', 9: 'ladder', 10: 'butterfly', 11: 'bicycle', 12: 'computer', 13: 'hat', 14: 'bee', 15: 'hamburger', 16: 'guitar', 17: 'cat', 18: 'leaf', 19: 'pencil', 20: 'chair', 21: 'star', 22: 'shoe', 23: 'sword', 24: 'scissors', 25: 'lion', 26: 'rabbit', 27: 'The_Eiffel_Tower', 28: 'hand', 29: 'tree'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5Dv0R3_zXhG",
        "outputId": "08d34339-0f2a-4f0e-8dac-71e030539971"
      },
      "source": [
        "for i in class_paths2:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "computer\n",
            "guitar\n",
            "hamburger\n",
            "hammer\n",
            "lion\n",
            "scissors\n",
            "sword\n",
            "star\n",
            "pencil\n",
            "rabbit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55OgwIA2zZZM"
      },
      "source": [
        "def draw_it(strokes):\n",
        "  image = Image.new(\"P\", (256,256), color=255)\n",
        "  image_draw = ImageDraw.Draw(image)\n",
        "  for stroke in ast.literal_eval(strokes):\n",
        "    for i in range(len(stroke[0])-1):\n",
        "      image_draw.line([stroke[0][i],\n",
        "                       stroke[1][i],\n",
        "                       stroke[0][i+1],\n",
        "                       stroke[1][i+1]],\n",
        "                       fill=0, width=5)\n",
        "  image = image.resize((imheight, imwidth))\n",
        "  return 255-np.array(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBKYzSwyzfNy"
      },
      "source": [
        "class DCGAN():\n",
        "  def __init__(self):\n",
        "      self.img_rows = imheight\n",
        "      self.img_cols = imwidth\n",
        "      self.img_shape = (self.img_rows, self.img_cols, 1)\n",
        "      self.latent_dim = 100\n",
        "      self.optimizer = Adam(lr=0.0002, beta_1=0.5, decay=8e-8)\n",
        "\n",
        "      # Build and compile the discriminator\n",
        "      self.discriminator = self.build_discriminator()\n",
        "      self.discriminator.compile(loss='binary_crossentropy', optimizer=self.optimizer)\n",
        "\n",
        "      # Build and compile the generator\n",
        "      self.generator = self.build_generator()\n",
        "      self.generator.compile(loss='binary_crossentropy', optimizer=self.optimizer)\n",
        "      self.gan = self.build_GAN()\n",
        "\n",
        "  def build_GAN(self):\n",
        "      self.discriminator.trainable = False\n",
        "      gan_input = Input(shape=(self.latent_dim,))\n",
        "      img = self.generator(gan_input)\n",
        "      gan_output = self.discriminator(img)\n",
        "      gan = Model(gan_input, gan_output, name='GAN')\n",
        "      gan.compile(loss='binary_crossentropy', optimizer=self.optimizer)\n",
        "      gan.summary()\n",
        "      return gan\n",
        "\n",
        "  def build_generator(self):\n",
        "      generator = Sequential()\n",
        "      generator.add(Dense(128 * 16 * 16, activation=\"relu\", input_dim=self.latent_dim))\n",
        "      generator.add(Reshape((16, 16, 128)))\n",
        "      generator.add(BatchNormalization(momentum=0.8))\n",
        "      generator.add(UpSampling2D())\n",
        "      generator.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
        "      generator.add(LeakyReLU(0.2))\n",
        "      generator.add(BatchNormalization(momentum=0.8))\n",
        "      generator.add(UpSampling2D())\n",
        "      generator.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
        "      generator.add(LeakyReLU(0.2))\n",
        "      generator.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "      generator.add(Conv2D(1, kernel_size=3, padding='same', activation = \"tanh\"))\n",
        "      generator.summary()\n",
        "      noise = Input(shape=(self.latent_dim,))\n",
        "      img = generator(noise)\n",
        "      return Model(noise, img, name='Generator')\n",
        "\n",
        "  def build_discriminator(self):\n",
        "      discriminator = Sequential()\n",
        "      discriminator.add(Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='same',\n",
        "                               input_shape=self.img_shape, kernel_initializer=RandomNormal(stddev=0.02)))\n",
        "      discriminator.add(LeakyReLU(0.2))\n",
        "      discriminator.add(Dropout(0.05))\n",
        "      discriminator.add(Conv2D(128, kernel_size=(5, 5), strides=(2, 2), padding='same'))\n",
        "      discriminator.add(LeakyReLU(0.2))\n",
        "      discriminator.add(Dropout(0.05))\n",
        "\n",
        "      discriminator.add(Conv2D(256, kernel_size=(5, 5), strides=(2, 2), padding='same'))\n",
        "      discriminator.add(LeakyReLU(0.2))\n",
        "      discriminator.add(Dropout(0.2))\n",
        "\n",
        "      discriminator.add(Flatten())\n",
        "      discriminator.add(Dense(1, activation='sigmoid'))\n",
        "      discriminator.summary()\n",
        "      img = Input(shape=self.img_shape)\n",
        "      validity = discriminator(img)\n",
        "      return Model(img, validity, name='Discriminator')\n",
        "\n",
        "\n",
        "  def train(self, X_train, epochs, batch_size=128, sample_interval=50):\n",
        "      ##self.generator = load_model(generator_path)\n",
        "      ##self.discriminator = load_model(discriminator_path)\n",
        "      real = np.ones((batch_size, 1))\n",
        "      fake = np.zeros((batch_size, 1))\n",
        "      avg_losses = {'D': [], 'G': []}\n",
        "      num_batches = len(X_train) // batch_size\n",
        "      for epoch in range(epochs):\n",
        "          d_loss_acc = 0\n",
        "          g_loss_acc = 0\n",
        "          for i in range(num_batches):\n",
        "            #Random batch of real doodles\n",
        "            imgs = X_train[np.random.randint(0, len(X_train) - 1, batch_size)]\n",
        "            #Generate a batch of doodles\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            gen_imgs = self.generator.predict(noise)\n",
        "            self.discriminator.trainable = True\n",
        "            #Train the discriminator (add random smoothing to labels)\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs, real * (np.random.uniform(0.7, 1.2)))\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake + (np.random.uniform(0.0, 0.3)))\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "            #We don't want the discriminator to get updated while the generator is being trained\n",
        "            self.discriminator.trainable = False\n",
        "            #Train the generator\n",
        "            g_loss = self.gan.train_on_batch(noise, real)\n",
        "            d_loss_acc += d_loss\n",
        "            g_loss_acc += g_loss\n",
        "          if (epoch + 1) % sample_interval == 0 or epoch == 0:\n",
        "              self.sample_images(epoch)\n",
        "          if (epoch + 1) % 3 == 0 or epoch == 0:\n",
        "              self.generator.save(generator_path)\n",
        "              self.discriminator.save(discriminator_path)\n",
        "\n",
        "          avg_losses['D'].append(d_loss_acc/num_batches)\n",
        "          avg_losses['G'].append(g_loss_acc/num_batches)\n",
        "      self.plot_loss(avg_losses)\n",
        "\n",
        "  def sample_images(self, epoch):\n",
        "    num_examples = 100\n",
        "    random_noise = np.random.normal(0, 1, size=[num_examples, self.latent_dim])\n",
        "    generated_images = self.generator.predict(random_noise)\n",
        "    generated_images = generated_images.reshape(num_examples, 64, 64)*127.5 + 1\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        plt.subplot(10, 10, i+1)\n",
        "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')\n",
        "        plt.axis('off')\n",
        "    plt.suptitle(\"Samples from G - Epoch = \" + str(epoch+1))\n",
        "    plt.show()\n",
        "\n",
        "  def plot_loss(self, losses):\n",
        "      plt.figure(figsize=(10, 8))\n",
        "      plt.plot(losses[\"D\"], label=\"Discriminator loss\")\n",
        "      plt.plot(losses[\"G\"], label=\"Generator loss\")\n",
        "      plt.xlabel('Epochs')\n",
        "      plt.ylabel('Loss')\n",
        "      plt.legend()\n",
        "      plt.title(\"Loss History\")\n",
        "      plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shNV-fmLzgDx"
      },
      "source": [
        "def csvWriter(file_name, nparray):\n",
        "  example = nparray.tolist()\n",
        "  with open(file_name+'.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',')\n",
        "    writer.writerows(example)\n",
        "\n",
        "def save_images(grapharray):\n",
        "  grapharray = np.reshape(grapharray, (64, 64))\n",
        "  plt.figure(figsize=(16, 16))\n",
        "  plt.imshow(grapharray, interpolation='nearest', cmap='gray_r')\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.savefig(output_image_dir + image_label + \"/\" + image_label + \"_{:04d}.png\".format(i) )\n",
        "  csvWriter(output_csv_dir + image_label + \"/\" + image_label + \"_{:04d}\".format(i), grapharray)\n",
        "def top_3_accuracy(x,y):\n",
        "  t3 = top_k_categorical_accuracy(x,y, 3)\n",
        "  return t3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1vNnoizh9u"
      },
      "source": [
        "for image_label in class_paths2:\n",
        "  num_classes = 30\n",
        "  imheight, imwidth = 64, 64\n",
        "  ims_per_class = 10000\n",
        "  #image_label = \"airplane\"\n",
        "  generator_path = \"/content/drive/My Drive/AI/output/model/generator/\" + image_label + \"_generator.h5\"\n",
        "  discriminator_path = \"/content/drive/My Drive/AI/output/model/discriminator/\" + image_label + \"_discriminator.h5\"\n",
        "  generator_test = pd.read_csv(\"/content/drive/My Drive/AI/input/train/\" + image_label + \".csv\",nrows=ims_per_class)\n",
        "  generator_imagebag = bag.from_sequence(generator_test.drawing.values).map(draw_it)\n",
        "  generator_trainarray = np.array(generator_imagebag.compute())\n",
        "\n",
        "  generator_X_train = ((generator_trainarray.astype(np.float32)/127.5)) - 1. #normalize to be [-1,1]\n",
        "  generator_num_train = generator_X_train.shape[0]\n",
        "  generator_X_train = generator_X_train.reshape(generator_num_train, imheight, imwidth, 1)\n",
        "\n",
        "  random_test = generator_X_train[np.random.randint(generator_num_train-1)].reshape(64, 64)\n",
        "  random_test = random_test*127.5 + 1\n",
        "  plt.imshow(random_test)\n",
        "\n",
        "  gan = DCGAN()\n",
        "  gan.train(generator_X_train, epochs=50, batch_size=128, sample_interval=5)\n",
        "\n",
        "  model_path = \"/content/drive/My Drive/AI/output/model/classifier.h5\"\n",
        "  output_csv_dir = \"/content/drive/My Drive/AI/output/csv/\"\n",
        "  output_image_dir = \"/content/drive/My Drive/AI/output/image/\"\n",
        "  model = keras.models.load_model(model_path, custom_objects={'top_3_accuracy': top_3_accuracy})\n",
        "  #image_label = \"airplane\"\n",
        "  img_h, img_w = 64, 64\n",
        "  num_examples = 100\n",
        "\n",
        "  gen_model = keras.models.load_model(\"/content/drive/My Drive/AI/output/model/generator/\"+image_label+ \"_generator.h5\")\n",
        "\n",
        "  correct_label = image_label.replace(\" \", \"_\")\n",
        "  print(correct_label)\n",
        "  latent_dim = 100\n",
        "  random_noise = np.random.normal(0, 1, size=[num_examples, latent_dim])\n",
        "  generator_predictions = gen_model.predict(random_noise)\n",
        "  generator_predictions = generator_predictions.reshape(num_examples, img_h, img_w)\n",
        "\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  correct_count = 0\n",
        "\n",
        "  for i in range(generator_predictions.shape[0]):\n",
        "      plt.subplot(10, 10, i+1)\n",
        "      plt.imshow(generator_predictions[i], interpolation='nearest', cmap='gray')\n",
        "      grapharray = generator_predictions[i]\n",
        "      for j in range(64):\n",
        "          for k in range(64):\n",
        "              if grapharray[j][k] < 0.2 : grapharray[j][k] = 1.\n",
        "              else: grapharray[j][k] = 0.\n",
        "\n",
        "      grapharray = np.reshape(grapharray, (1, 64, 64, 1))\n",
        "      graphpreds = model.predict(grapharray, verbose=0)\n",
        "      gvs = np.argsort(-graphpreds)\n",
        "\n",
        "      try:\n",
        "        os.mkdir(output_image_dir + image_label)\n",
        "      except:\n",
        "        print('fail')\n",
        "      try:\n",
        "        os.mkdir(output_csv_dir + image_label)\n",
        "      except:\n",
        "        print('fail')\n",
        "\n",
        "\n",
        "      for idx in gvs[:]:\n",
        "          print(numstonames[idx[0]], numstonames[idx[1]], numstonames[idx[2]])\n",
        "          if numstonames[idx[0]] == correct_label :\n",
        "            correct_count = correct_count + 1\n",
        "            save_images(grapharray)\n",
        "      plt.axis('off')\n",
        "\n",
        "  print(\"Accuracy = \", correct_count / num_examples)\n",
        "\n",
        "  plt.suptitle(\"Samples\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBHef1jrz12q"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}